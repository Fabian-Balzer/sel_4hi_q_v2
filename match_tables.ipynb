{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to match the relevant tables for 4Hi-q\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To be able to run this, the following requirements need to be met:\n",
    "\n",
    "- A `python 3.8.10` environment, including the `astropy`, `astroquery`, `scipy` and standard modules\n",
    "- The `function package` and the `catalogues` in the same directory as this script.\n",
    "\n",
    "## Structure\n",
    "\n",
    "This script consists of a few separate parts to handle the catalogue data, divided into the following sections:\n",
    "\n",
    "- Querying for the catalogue data and writing backups to disk, requiring the following:\n",
    "  - A `region` specified in the beginning, used for RA and DEC constraints\n",
    "  - An `agn_cat`, providing RA, DEC and probability information on the AGNs\n",
    "  - The `sweep_cat` catalogues for the region in question (TODO: Use the LS DR query interface)\n",
    "  - The `vhs_cat` is obtained by querying the Vista Science Archive\n",
    "- Matching the catalogues, also writing a backup.\n",
    "  - The matching is performed on the respective RA and DEC coordinates (TODO: Use the advanced constraints described in my thesis)\n",
    "  - A match via the `cds` service is performed with the GALEX catalogue\n",
    "- Processing the catalogues (unit conversions and reddening correction), including a split into *pointlike* and *extended*.\n",
    "- Collapsing everything into a `LePhare` input file\n",
    "  \n",
    "Note: After running this script successfully, the photometric redshift routine can be carried out using `run_lephare.ipynb`\n",
    "\n",
    "## Citations\n",
    "\n",
    "- Please see `other/citations.md` for any citations/acknowledgements that might be required when using this script.\n",
    "\n",
    "All of the functions used for these routines can either be found directly in this notebook or in the `function_package` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Modules loaded successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not import regions, which is required for some of the functionalities of this module.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import function_package as fp\n",
    "import logging\n",
    "logging.getLogger().setLevel(\"INFO\")\n",
    "\n",
    "logging.info(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the directories necessary\n",
    "fp.generate_all_filepaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Region constrained to 125.0000 <= RA <= 130.0000 and -5.0000 <= DEC <= 0.0000.\n",
      "This corresponds to a linear (!) size of 25 deg^2\n"
     ]
    }
   ],
   "source": [
    "# Define the region to constrain everything to:\n",
    "# The eFEDs field is dec < 6.2 & dec > -3.2 & ra < 146.2 & ra > 126.\n",
    "ra_min, ra_max, dec_min, dec_max = 125, 130, -5, 0\n",
    "REGION = fp.Region(ra_min, ra_max, dec_min, dec_max)\n",
    "logging.info(REGION)\n",
    "REGION.save_to_disk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the tables\n",
    "\n",
    "In this section, the tables are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The reduced shu_agn table provided contains 2437 sources.\n",
      "INFO:root:After the probability cut at p_rf >= 0.940, 1899 sources are left in the shu_agn table\n"
     ]
    }
   ],
   "source": [
    "SHU_TABLE = fp.load_and_clean_opt_agn_shu(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The vhs table provided contains 1547498 sources.\n",
      "INFO:root:The reduced vhs table provided contains 291905 sources.\n"
     ]
    }
   ],
   "source": [
    "VHS_TABLE = fp.load_and_clean_vhs(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The following bricks are in the requested region for the sweep table:\n",
      "['120m005-130p000']\n",
      "INFO:root:The reduced sweep table provided contains 2421599 sources.\n"
     ]
    }
   ],
   "source": [
    "SWEEP_TABLE = fp.load_and_clean_sweep(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Now that the tables are properly loaded (except for GALEX, which we'll acquire while matching), we can match them one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Match of agn to sweep successful;\n",
      "For 0 of the 1899 sources in the agn table, no match was found within the given radius.\n",
      "INFO:root:From now on, the ra and dec columns refer to the sweep ra and dec.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1899\n"
     ]
    }
   ],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius between sweep and the shu et al. agn table:\n",
    "match_radius_shu = 0.1\n",
    "MATCH_TABLE = fp.match_shu_with_sweep(SWEEP_TABLE, SHU_TABLE, match_radius=match_radius_shu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 683 matching vhs sources within the prescribed radius.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1899"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius to vhs:\n",
    "match_radius_vhs = 0.19\n",
    "MATCH_TABLE2 = fp.match_vhs_to_table(MATCH_TABLE, VHS_TABLE, match_radius=match_radius_vhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 526 matching galex sources within the prescribed radius.\n"
     ]
    }
   ],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius to galex:\n",
    "match_radius_galex = 2.1\n",
    "MATCH_TABLE3 = fp.match_with_galex_and_clean_it(MATCH_TABLE2, match_radius=match_radius_galex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully written a match_backup file at /hshome/bat3083/sel_4hi_q_v2/data/match_backups/base_full_match.fits.\n"
     ]
    }
   ],
   "source": [
    "# Write a backup of the matched table to disk\n",
    "fp.write_table_as_backup(MATCH_TABLE3, \"match_backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "The matched table can now be processed.\\\n",
    "For this, all columns are turned into corrected magnitude ones, and the table is split up into a *pointlike* and an *extended* part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hslxrsrv3/bat3083/.local/lib/python3.8/site-packages/numpy/ma/core.py:6849: RuntimeWarning: overflow encountered in power\n",
      "  result = np.where(m, fa, umath.power(fa, fb)).view(basetype)\n",
      "INFO:root:Successfully written a processed_backup file at /hshome/bat3083/sel_4hi_q_v2/data/match_backups/base_processed_pointlike.fits.\n",
      "INFO:root:Successfully written a processed_backup file at /hshome/bat3083/sel_4hi_q_v2/data/match_backups/base_processed_extended.fits.\n"
     ]
    }
   ],
   "source": [
    "MATCH_TABLE3 = fp.read_table_from_backup(\"match_backup\")\n",
    "PROCESSED = fp.process_galex_columns(MATCH_TABLE3)\n",
    "PROCESSED = fp.process_sweep_columns(PROCESSED)\n",
    "POINTLIKE, EXTENDED = fp.split_table_by_sourcetype(PROCESSED)\n",
    "POINTLIKE = fp.process_vhs_columns(POINTLIKE)\n",
    "EXTENDED = fp.process_vhs_columns(EXTENDED)\n",
    "fp.write_table_as_backup(POINTLIKE, \"processed_backup\", \"pointlike\")\n",
    "fp.write_table_as_backup(EXTENDED, \"processed_backup\", \"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LePhare input files\n",
    "\n",
    "Now that we do have the pointlike and extended subsets, we can create the LePhare input table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully written a lephare_in file at /hshome/bat3083/sel_4hi_q_v2/data/lephare/input/base_input_pointlike.in.\n",
      "INFO:root:Successfully written a lephare_in file at /hshome/bat3083/sel_4hi_q_v2/data/lephare/input/base_input_extended.in.\n"
     ]
    }
   ],
   "source": [
    "POINTLIKE_LEPHARE = fp.process_for_lephare(POINTLIKE)\n",
    "EXTENDED_LEPHARE = fp.process_for_lephare(EXTENDED)\n",
    "fp.write_table_as_backup(POINTLIKE_LEPHARE, \"lephare_in\", \"pointlike\", overwrite=True)\n",
    "fp.write_table_as_backup(EXTENDED_LEPHARE, \"lephare_in\", \"extended\", overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
