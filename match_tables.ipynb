{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to match the relevant tables for 4Hi-q\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To be able to run this, the following requirements need to be met:\n",
    "\n",
    "- A `python 3.10` environment, including the `astropy`, `astroquery` and standard modules\n",
    "- The `function package` and the `catalogues` in the same directory.\n",
    "\n",
    "## Structure\n",
    "\n",
    "This script consists of a few separate parts to handle the catalogue data, divided into the following sections:\n",
    "\n",
    "- Querying for the catalogue data and writing backups to disk, requiring the following:\n",
    "  - A `region` specified in the beginning, used for RA and DEC constraints\n",
    "  - An `agn_cat`, providing RA, DEC and probability information on the AGNs\n",
    "  - The `sweep_cat` catalogues for the region in question (TODO: Use the LS DR query interface)\n",
    "  - The `vhs_cat` is obtained by querying the Vista Science Archive\n",
    "- Matching the catalogues, also writing a backup.\n",
    "  - The matching is performed on the respective RA and DEC coordinates (TODO: Use the advanced constraints described in my thesis)\n",
    "  - A match via the `cds` service is performed with the GALEX catalogue\n",
    "- Processing the catalogues (unit conversions and reddening correction), including a split into *pointlike* and *extended*.\n",
    "- Collapsing everything into a `LePhare` input file\n",
    "- Running `LePhare`:\n",
    "  - Collect the filter information\n",
    "  - Collect the template information\n",
    "  - Run the photo-z-routine\n",
    "\n",
    "All of the functions used for these routines can either be found directly in this notebook or in the `function_package` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Modules loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import function_package as fp\n",
    "import logging\n",
    "logging.getLogger().setLevel(\"INFO\")\n",
    "\n",
    "logging.info(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the directories necessary\n",
    "fp.generate_all_filepaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Region constrained to 125.0000 <= RA <= 130.0000 and -5.0000 <= DEC <= 0.0000.\n",
      "This corresponds to a linear (!) size of 25 deg^2\n"
     ]
    }
   ],
   "source": [
    "# Define the region to constrain everything to:\n",
    "# The eFEDs field is dec < 6.2 & dec > -3.2 & ra < 146.2 & ra > 126.\n",
    "ra_min, ra_max, dec_min, dec_max = 125, 130, -5, 0\n",
    "REGION = fp.Region(ra_min, ra_max, dec_min, dec_max)\n",
    "logging.info(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the tables\n",
    "\n",
    "In this section, the tables are loaded.\\\n",
    "TODO: Make use of astroquery if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The reduced shu_agn table provided contains 2437 sources.\n",
      "INFO:root:After the probability cut at p_rf >= 0.940, 1899 sources are left in the shu_agn table\n"
     ]
    }
   ],
   "source": [
    "SHU_TABLE = fp.load_and_clean_opt_agn_shu(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The vhs table provided contains 1547498 sources.\n",
      "INFO:root:The reduced vhs table provided contains 291905 sources.\n"
     ]
    }
   ],
   "source": [
    "VHS_TABLE = fp.load_and_clean_vhs(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The following bricks are in the requested region for the sweep table:\n",
      "['120m005-130p000']\n",
      "INFO:root:The reduced sweep table provided contains 2421599 sources.\n"
     ]
    }
   ],
   "source": [
    "SWEEP_TABLE = fp.load_and_clean_sweep(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Now that the tables are properly loaded (except for GALEX, which we'll acquire while matching), we can match them one by one.\\\n",
    "TODO: Implement the functions if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MATCH_TABLE = fp.match_with_sweep(SHU_TABLE, SWEEP_TABLE)\n",
    "# # TODO: Implement matching.\n",
    "# MATCH_TABLE = fp.match_with_vhs(MATCH_TABLE, VHS_TABLE)\n",
    "# MATCH_TABLE = fp.match_with_galex(MATCH_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "The matched table can now be processed.\\\n",
    "For this, all columns are turned into corrected magnitude ones, and the table is split up into a *pointlike* and an *extended* part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:The reduced matched table provided contains 975 sources.\n",
      "INFO:root:The reduced matched table provided contains 903 sources.\n"
     ]
    }
   ],
   "source": [
    "from astropy.table import Table, join\n",
    "import warnings\n",
    "from astropy.units import UnitsWarning\n",
    "from function_package.load_and_clean_tables import _sanitise_table\n",
    "import numpy as np\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UnitsWarning)\n",
    "    MATCH_TABLE = Table.read(f\"{fp.DATAPATH}match_backups/baseline_input_raw_match.fits\")\n",
    "MATCH_TABLE = _sanitise_table(MATCH_TABLE, REGION, \"matched\")\n",
    "MATCH_TABLE.rename_column(\"type\", \"sweep_type\")\n",
    "MATCH_TABLE.rename_column(\"sweep_ident\", \"sweep_id\")\n",
    "MATCH_TABLE.rename_columns([\"fflux\", \"nflux\", \"e_fflux\", \"e_nflux\"], [\"flux_fuv\", \"flux_nuv\", \"flux_err_fuv\", \"flux_err_nuv\"])\n",
    "MATCH_TABLE.rename_column(\"ebv_galex\", \"galex_ebv\")\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\", UnitsWarning)\n",
    "    POINTLIKE_COMP = Table.read(f\"{fp.DATAPATH}match_backups/baseline_input_pointlike_processed.fits\")\n",
    "POINTLIKE_COMP = _sanitise_table(POINTLIKE_COMP, REGION, \"matched\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:As we can see, the relative errors are neglectible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g\n",
      "1.1616040043749564e-07\n",
      "r\n",
      "1.0551129522971926e-07\n",
      "z\n",
      "1.0810402448477127e-07\n",
      "w1\n",
      "1.0754281431780065e-07\n",
      "w2\n",
      "1.0678235412312255e-07\n",
      "w3\n",
      "1.0759226515561465e-07\n",
      "w4\n",
      "1.0884313731172959e-07\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the new sweep processing:\n",
    "SWEEP_CORR = fp.process_sweep_columns(MATCH_TABLE)\n",
    "plike, ext = fp.split_table_by_sourcetype(SWEEP_CORR)\n",
    "band = \"g\"\n",
    "from function_package.custom_constants import ALL_SWEEP_BANDS\n",
    "for band in ALL_SWEEP_BANDS:\n",
    "    print(band)\n",
    "    print(np.max((plike[f\"c_flux_{band}\"] - POINTLIKE_COMP[f\"c_flux_{band}\"])/ plike[f\"c_flux_{band}\"]))\n",
    "logging.info(\"As we can see, the relative errors are neglectible.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:As we can see, the relative errors are neglectible, but we need to watch out for NAN values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fuv\n",
      "1.4809559830549247e-07\n",
      "nuv\n",
      "1.1365383890635863e-07\n"
     ]
    }
   ],
   "source": [
    "# Test the performance of the new Galex processing\n",
    "try:\n",
    "    MATCH_TABLE.rename_columns([\"fflux\", \"nflux\", \"e_fflux\", \"e_nflux\"], [\"flux_fuv\", \"flux_nuv\", \"flux_err_fuv\", \"flux_err_nuv\"])\n",
    "    MATCH_TABLE.rename_column(\"ebv_galex\", \"galex_ebv\")\n",
    "except KeyError:\n",
    "    pass\n",
    "GALEX_CORR = fp.process_galex_columns(MATCH_TABLE)\n",
    "plike, ext = fp.split_table_by_sourcetype(GALEX_CORR)\n",
    "from function_package.custom_constants import ALL_GALEX_BANDS\n",
    "for band in ALL_GALEX_BANDS:\n",
    "    mask = POINTLIKE_COMP[f\"c_flux_{band}\"] > 0\n",
    "    print(band)\n",
    "    print(np.max((plike[mask][f\"c_flux_{band}\"] - POINTLIKE_COMP[mask][f\"c_flux_{band}\"])/ plike[mask][f\"c_flux_{band}\"]))\n",
    "logging.info(\"As we can see, the relative errors are neglectible, but we need to watch out for NAN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:As we can see, the relative errors are neglectible, but we need to watch out for NAN values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "j\n",
      "7.4732438287794575e-06\n",
      "h\n",
      "ks\n",
      "7.59225810582596e-06\n"
     ]
    }
   ],
   "source": [
    "plike, ext = fp.split_table_by_sourcetype(MATCH_TABLE)\n",
    "VHS_PLIKE = fp.process_vhs_columns(plike)\n",
    "from function_package.custom_constants import ALL_VHS_BANDS\n",
    "for band in ALL_VHS_BANDS:\n",
    "    mask = POINTLIKE_COMP[f\"c_flux_{band}\"] > 0\n",
    "    print(band)\n",
    "    if np.sum(mask) > 0:\n",
    "        print(np.max((VHS_PLIKE[mask][f\"c_flux_{band}\"] - POINTLIKE_COMP[mask][f\"c_flux_{band}\"])/ VHS_PLIKE[mask][f\"c_flux_{band}\"]))\n",
    "logging.info(\"As we can see, the relative errors are neglectible, but we need to watch out for NAN values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fabia\\anaconda3\\envs\\4HIQ\\lib\\site-packages\\numpy\\ma\\core.py:6900: RuntimeWarning: overflow encountered in power\n",
      "  result = np.where(m, fa, umath.power(fa, fb)).view(basetype)\n"
     ]
    }
   ],
   "source": [
    "PROCESSED = fp.process_galex_columns(MATCH_TABLE)\n",
    "PROCESSED = fp.process_sweep_columns(PROCESSED)\n",
    "POINTLIKE, EXTENDED = fp.split_table_by_sourcetype(PROCESSED)\n",
    "POINTLIKE = fp.process_vhs_columns(POINTLIKE)\n",
    "EXTENDED = fp.process_vhs_columns(EXTENDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Photometry for problematic matches should be discarded!\n",
    "# For this, you need to look at the distance between the sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LePhare input files\n",
    "\n",
    "Now that we do have the pointlike and extended subsets, we can create the LePhare input table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully written a LePhare input file at c:\\Users\\fabia\\OneDrive\\Dokumente\\Studium\\Master\\Thesis\\LePhare_work\\new_attempt/data/lephare_in/base_input_pointlike as a .fits and as a .in\n",
      "INFO:root:Successfully written a LePhare input file at c:\\Users\\fabia\\OneDrive\\Dokumente\\Studium\\Master\\Thesis\\LePhare_work\\new_attempt/data/lephare_in/base_input_extended as a .fits and as a .in\n"
     ]
    }
   ],
   "source": [
    "POINTLIKE_LEPHARE = fp.process_for_lephare(POINTLIKE)\n",
    "EXTENDED_LEPHARE = fp.process_for_lephare(EXTENDED)\n",
    "fp.write_lephare_input(POINTLIKE_LEPHARE, \"pointlike\", overwrite=True)\n",
    "fp.write_lephare_input(EXTENDED_LEPHARE, \"extended\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LePhare\n",
    "\n",
    "Before we run any of the subprograms, we need to initialise important paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_STEM = \"base\"\n",
    "PARA_STEM = \"base\"\n",
    "INPUT_STEM = \"base\"\n",
    "\n",
    "PATH_PARA_IN = fp.get_filepath(\"para_in\")\n",
    "PATH_FILTER_REPO = fp.get_filepath(\"filter_repo\")\n",
    "PATH_FILTER_FILE = fp.get_filepath(\"filter\", FILTER_STEM)\n",
    "PATH_LEPHAREDIR = fp.get_filepath(\"lepharedir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) Run the filter file\n",
    "\n",
    "This step can be skipped if the filterfile has already been set up before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'function_package' has no attribute 'get_filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [180], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fpath_para \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mget_filepath(\u001b[39m\"\u001b[39m\u001b[39mpara\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m fpath_filter_repo \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mget_filepath(\u001b[39m\"\u001b[39m\u001b[39mfilter_repo\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m filter_stem \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbase\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'function_package' has no attribute 'get_filepath'"
     ]
    }
   ],
   "source": [
    "!{$fpath_lephare}/source/filter -c $fpath_para \\\n",
    "    -FILTER_REP $fpath_filter_repo \\\n",
    "    -FILTER_FILE $filter_stem \\\n",
    "    >$fpath_filter_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Run the template files\n",
    "\n",
    "This step can be skipped if the required template files already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Run the photo-z routine\n",
    "\n",
    "This is the core functionality of LePhare.\\\n",
    "Be sure you know what you're doing when modifying the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('4HIQ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5467bde37112203a91ed13ee214a716a4e450c78160f6a2777db205ddeaee664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
