{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to match the relevant tables for 4Hi-q\n",
    "\n",
    "## Requirements\n",
    "\n",
    "To be able to run this, the following requirements need to be met:\n",
    "\n",
    "- A `python 3.10` environment, including the `astropy`, `astroquery`, `scipy` and standard modules\n",
    "- The `function package` and the `catalogues` in the same directory as this script.\n",
    "\n",
    "## Structure\n",
    "\n",
    "This script consists of a few separate parts to handle the catalogue data, divided into the following sections:\n",
    "\n",
    "- Querying for the catalogue data and writing backups to disk, requiring the following:\n",
    "  - A `region` specified in the beginning, used for RA and DEC constraints\n",
    "  - An `agn_cat`, providing RA, DEC and probability information on the AGNs\n",
    "  - The `sweep_cat` catalogues for the region in question (TODO: Use the LS DR query interface)\n",
    "  - The `vhs_cat` is obtained by querying the Vista Science Archive\n",
    "- Matching the catalogues, also writing a backup.\n",
    "  - The matching is performed on the respective RA and DEC coordinates (TODO: Use the advanced constraints described in my thesis)\n",
    "  - A match via the `cds` service is performed with the GALEX catalogue\n",
    "- Processing the catalogues (unit conversions and reddening correction), including a split into *pointlike* and *extended*.\n",
    "- Collapsing everything into a `LePhare` input file\n",
    "- Running `LePhare`:\n",
    "  - Collect the filter information\n",
    "  - Collect the template information\n",
    "  - Run the photo-z-routine\n",
    "\n",
    "## Citations\n",
    "\n",
    "- Please see `other/citations.md` for any citations/acknowledgements that might be required when using this script.\n",
    "\n",
    "All of the functions used for these routines can either be found directly in this notebook or in the `function_package` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import function_package as fp\n",
    "import logging\n",
    "logging.getLogger().setLevel(\"INFO\")\n",
    "\n",
    "logging.info(\"Modules loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the directories necessary\n",
    "fp.generate_all_filepaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the region to constrain everything to:\n",
    "# The eFEDs field is dec < 6.2 & dec > -3.2 & ra < 146.2 & ra > 126.\n",
    "ra_min, ra_max, dec_min, dec_max = 125, 130, -5, 0\n",
    "REGION = fp.Region(ra_min, ra_max, dec_min, dec_max)\n",
    "logging.info(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the tables\n",
    "\n",
    "In this section, the tables are loaded.\\\n",
    "TODO: Make use of astroquery if possible\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHU_TABLE = fp.load_and_clean_opt_agn_shu(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VHS_TABLE = fp.load_and_clean_vhs(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SWEEP_TABLE = fp.load_and_clean_sweep(REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching\n",
    "\n",
    "Now that the tables are properly loaded (except for GALEX, which we'll acquire while matching), we can match them one by one.\\\n",
    "TODO: Implement the functions if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Match of agn to sweep successful;\n",
      "For 0 of the 1899 sources in the agn table, no match was found within the given radius.\n",
      "INFO:root:From now on, the ra and dec columns refer to the sweep ra and dec.\n"
     ]
    }
   ],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius between sweep and the shu et al. agn table:\n",
    "match_radius_shu = 0.1\n",
    "MATCH_TABLE = fp.match_shu_with_sweep(SWEEP_TABLE, SHU_TABLE, match_radius=match_radius_shu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Found 683 matching vhs sources within the prescribed radius.\n"
     ]
    }
   ],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius to vhs:\n",
    "match_radius_vhs = 0.19\n",
    "MATCH_TABLE2 = fp.match_vhs_to_table(MATCH_TABLE, VHS_TABLE, match_radius=match_radius_vhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In my master thesis, I found the following to be a reasonable matching radius to galex:\n",
    "match_radius_galex = 2.1\n",
    "MATCH_TABLE3 = fp.match_with_galex_and_clean_it(MATCH_TABLE2, match_radius=match_radius_galex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Successfully written a match_backup file at c:\\Users\\fabia\\OneDrive\\Dokumente\\Studium\\Master\\Thesis\\LePhare_work\\new_attempt/data/match_backups/base_full_match.fits.\n"
     ]
    }
   ],
   "source": [
    "# Write a backup of the matched table to disk\n",
    "fp.write_table_as_backup(MATCH_TABLE3, \"match_backup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing\n",
    "\n",
    "The matched table can now be processed.\\\n",
    "For this, all columns are turned into corrected magnitude ones, and the table is split up into a *pointlike* and an *extended* part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'flux_fuv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [40], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m MATCH_TABLE3 \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mread_table_from_backup(\u001b[39m\"\u001b[39m\u001b[39mmatch_backup\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m PROCESSED \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39;49mprocess_galex_columns(MATCH_TABLE3)\n\u001b[0;32m      3\u001b[0m PROCESSED \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39mprocess_sweep_columns(PROCESSED)\n\u001b[0;32m      4\u001b[0m POINTLIKE, EXTENDED \u001b[39m=\u001b[39m fp\u001b[39m.\u001b[39msplit_table_by_sourcetype(PROCESSED)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\OneDrive\\Dokumente\\Studium\\Master\\Thesis\\LePhare_work\\new_attempt\\function_package\\pre_processing.py:79\u001b[0m, in \u001b[0;36mprocess_galex_columns\u001b[1;34m(table, bands)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39m\"\"\"Correct the galex columns (assumed to be of the form `flux_{band}` and `flux_err_{band}`)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mfor the EBV values that are provided in the `galex_ebv` column.\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[39m    The table with the value-added columns\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39mfor\u001b[39;00m band \u001b[39min\u001b[39;00m bands:\n\u001b[1;32m---> 79\u001b[0m     table \u001b[39m=\u001b[39m _process_single_galex_column(table, band)\n\u001b[0;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m table\n",
      "File \u001b[1;32mc:\\Users\\fabia\\OneDrive\\Dokumente\\Studium\\Master\\Thesis\\LePhare_work\\new_attempt\\function_package\\pre_processing.py:55\u001b[0m, in \u001b[0;36m_process_single_galex_column\u001b[1;34m(table, band)\u001b[0m\n\u001b[0;32m     53\u001b[0m correction_factors \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mfuv\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m8.06\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnuv\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m7.95\u001b[39m}\n\u001b[0;32m     54\u001b[0m corr_factor \u001b[39m=\u001b[39m correction_factors[band]\n\u001b[1;32m---> 55\u001b[0m table[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc_flux_\u001b[39m\u001b[39m{\u001b[39;00mband\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m table[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mflux_\u001b[39;49m\u001b[39m{\u001b[39;49;00mband\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m] \u001b[39m*\u001b[39m \\\n\u001b[0;32m     56\u001b[0m     \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(corr_factor \u001b[39m*\u001b[39m table[\u001b[39m\"\u001b[39m\u001b[39mgalex_ebv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m2.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m1e-29\u001b[39m\n\u001b[0;32m     57\u001b[0m table[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mc_flux_err_\u001b[39m\u001b[39m{\u001b[39;00mband\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m table[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mflux_err_\u001b[39m\u001b[39m{\u001b[39;00mband\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m] \u001b[39m*\u001b[39m \\\n\u001b[0;32m     58\u001b[0m     \u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m(corr_factor \u001b[39m*\u001b[39m table[\u001b[39m\"\u001b[39m\u001b[39mgalex_ebv\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m/\u001b[39m \u001b[39m2.5\u001b[39m) \u001b[39m*\u001b[39m \u001b[39m1e-29\u001b[39m\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m table\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\envs\\4HIQ\\lib\\site-packages\\astropy\\table\\table.py:1872\u001b[0m, in \u001b[0;36mTable.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1870\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[0;32m   1871\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[1;32m-> 1872\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns[item]\n\u001b[0;32m   1873\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger)):\n\u001b[0;32m   1874\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mRow(\u001b[39mself\u001b[39m, item)\n",
      "File \u001b[1;32mc:\\Users\\fabia\\anaconda3\\envs\\4HIQ\\lib\\site-packages\\astropy\\table\\table.py:246\u001b[0m, in \u001b[0;36mTableColumns.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[39m\"\"\"Get items from a TableColumns object.\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[39m::\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39m  tc[1:3] # <TableColumns names=('b', 'c')>\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, \u001b[39mstr\u001b[39m):\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m OrderedDict\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(\u001b[39mself\u001b[39;49m, item)\n\u001b[0;32m    247\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(item, (\u001b[39mint\u001b[39m, np\u001b[39m.\u001b[39minteger)):\n\u001b[0;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalues())[item]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'flux_fuv'"
     ]
    }
   ],
   "source": [
    "MATCH_TABLE3 = fp.read_table_from_backup(\"match_backup\")\n",
    "PROCESSED = fp.process_galex_columns(MATCH_TABLE3)\n",
    "PROCESSED = fp.process_sweep_columns(PROCESSED)\n",
    "POINTLIKE, EXTENDED = fp.split_table_by_sourcetype(PROCESSED)\n",
    "POINTLIKE = fp.process_vhs_columns(POINTLIKE)\n",
    "EXTENDED = fp.process_vhs_columns(EXTENDED)\n",
    "fp.write_table_as_backup(POINTLIKE, \"processed_backup\", \"pointlike\")\n",
    "fp.write_table_as_backup(EXTENDED, \"processed_backup\", \"extended\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create LePhare input files\n",
    "\n",
    "Now that we do have the pointlike and extended subsets, we can create the LePhare input table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTLIKE_LEPHARE = fp.process_for_lephare(POINTLIKE)\n",
    "EXTENDED_LEPHARE = fp.process_for_lephare(EXTENDED)\n",
    "fp.write_table_as_backup(POINTLIKE_LEPHARE, \"lephare_in\", \"pointlike\", overwrite=True)\n",
    "fp.write_table_as_backup(EXTENDED_LEPHARE, \"lephare_in\", \"extended\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LePhare\n",
    "\n",
    "Before we run any of the subprograms, we need to initialise important paths:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_STEM = \"base\"\n",
    "PARA_STEM = \"base\"\n",
    "INPUT_STEM = \"base\"\n",
    "\n",
    "PATH_PARA_IN = fp.get_filepath(\"para_in\")\n",
    "PATH_FILTER_REPO = fp.get_filepath(\"filter_repo\")\n",
    "PATH_FILTER_FILE = fp.get_filepath(\"filter\", FILTER_STEM)\n",
    "PATH_LEPHAREDIR = fp.get_filepath(\"lepharedir\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1) Run the filter file\n",
    "\n",
    "This step can be skipped if the filterfile has already been set up before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{$fpath_lephare}/source/filter -c $fpath_para \\\n",
    "    -FILTER_REP $fpath_filter_repo \\\n",
    "    -FILTER_FILE $filter_stem \\\n",
    "    >$fpath_filter_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Run the template files\n",
    "\n",
    "This step can be skipped if the required template files already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Run the photo-z routine\n",
    "\n",
    "This is the core functionality of LePhare.\\\n",
    "Be sure you know what you're doing when modifying the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('4HIQ')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5467bde37112203a91ed13ee214a716a4e450c78160f6a2777db205ddeaee664"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
